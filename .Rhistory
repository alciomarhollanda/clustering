}
library(reshape2)
dt <- melt(avgS)
colnames(dt) <- c("NClusts","Meth","AvgS")
library(ggplot2)
ggplot(dt,aes(x=NClusts,y=AvgS,color=Meth)) +
geom_line()
library(cluster)
d <- dist(scale(iris[,-5]))
methds <- c('kmeans','fanny','pam')
avgS <- matrix(NA,ncol=3,nrow=5,
dimnames=list(2:6,methds))
for(k in 2:6)
for(m in seq_along(methds)) {
if(m == 1){
c <- kmeans(d,centers=k,iter.max=200)
}else if( m == 2){
c <- fanny(d,k,metric='euclidean',stand=T)
}else if(m == 3){
c <- pam(d,k=k)
}
s <- silhouette(c$cluster,d)
avgS[k-1,m] <- mean(s[,3])
}
library(reshape2)
dt <- melt(avgS)
colnames(dt) <- c("NClusts","Meth","AvgS")
library(ggplot2)
ggplot(dt,aes(x=NClusts,y=AvgS,color=Meth)) +
geom_line()
library(cluster)
d <- dist(scale(iris[,-5]))
methds <- c('kmeans','fanny','pam')
avgS <- matrix(NA,ncol=3,nrow=5,
dimnames=list(2:6,methds))
for(k in 2:6)
for(m in seq_along(methds)) {
if(m == 1){
c <- kmeans(d,centers=k,iter.max=200)
}else if( m == 2){
c <- fanny(d,k,metric='euclidean',stand=T)
}else if(m == 3){
c <- pam(d,k=k)
}
s <- silhouette(c$cluster,d)
avgS[k-1,m] <- mean(s[,3])
}
library(reshape2)
dt <- melt(avgS)
colnames(dt) <- c("NClusts","Meth","AvgS")
library(ggplot2)
ggplot(dt,aes(x=NClusts,y=AvgS,color=Meth)) +
geom_line()
library(cluster)
d <- dist(scale(iris[,-5]))
methds <- c('kmeans','fanny','pam')
avgS <- matrix(NA,ncol=3,nrow=5,
dimnames=list(2:6,methds))
for(k in 2:6)
for(m in seq_along(methds)) {
if(m == 1){
c <- kmeans(d,centers=k,iter.max=200)
}else if( m == 2){
c <- fanny(d,k,metric='euclidean',stand=T)
}else if(m == 3){
c <- pam(d,k=k)
}
s <- silhouette(c$cluster,d)
avgS[k-1,m] <- mean(s[,3])
}
library(reshape2)
dt <- melt(avgS)
colnames(dt) <- c("NClusts","Meth","AvgS")
library(ggplot2)
ggplot(dt,aes(x=NClusts,y=AvgS,color=Meth)) +
geom_line()
library(cluster)
d <- dist(scale(iris[,-5]))
methds <- c('kmeans','fanny','pam')
avgS <- matrix(NA,ncol=3,nrow=5,
dimnames=list(2:6,methds))
for(k in 2:6)
for(m in seq_along(methds)) {
if(m == 1){
c <- kmeans(d,centers=k,iter.max=200)
}else if( m == 2){
c <- fanny(d,k,metric='euclidean',stand=T)
}else if(m == 3){
c <- pam(d,k=k)
}
s <- silhouette(c$cluster,d)
avgS[k-1,m] <- mean(s[,3])
}
library(reshape2)
dt <- melt(avgS)
colnames(dt) <- c("NClusts","Meth","AvgS")
library(ggplot2)
ggplot(dt,aes(x=NClusts,y=AvgS,color=Meth)) +
geom_line()
library(diceR)
library(dplyr)
library(diceR)
library(dplyr)
library(ggplot2)
library(pander)
data(hgsc)
hgsc <- hgsc[1:100, 1:50]
#agrupar os hgscdados em 3 ou 4 clusters, usando 80% de reimampling em 5 repeti??es,
#para esses algoritmos de cluster: Hierarchical Clustering, PAM e DIvisive
#ANAlysis Clustering (DIANA).
#A dist?ncia euclidiana ? utilizada para todos os algoritmos.
CC <- consensus_cluster(hgsc, nk = 3:4, p.item = 0.8, reps = 5,
algorithms = c("hc", "pam", "diana"))
co <- capture.output(str(CC))
strwrap(co, width = 80)
CC <- apply(CC, 2:4, impute_knn, data = hgsc, seed = 1)
bigger = options()$max.print  # or add something larger
options("max.print" = bigger) # apparently RStudio sets max.print very low.
CC
# Filtra somente PAM_Euclidean 4
pam.4 <- CC[, , "PAM_Euclidean", "4", drop = FALSE]
cm <- consensus_matrix(pam.4)
dim(cm)
hm <- graph_heatmap(pam.4)
dim(cm)
cm
pam.4
pam.4
x <- consensus_cluster(dat, nk = 3, reps = 10, algorithms = c("km","hc","diana"),
progress = FALSE)
library(diceR)
library(ggplot2)
dat <- iris[, -5]
x <- consensus_cluster(dat, nk = 3, reps = 10, algorithms = c("km","hc","diana"),
progress = FALSE)
knitr::kable(head(x$clusters))
knitr::kable(head(x$clusters))
x
mvData <- majority_voting(x,is.relabelled = FALSE)
mvData
knitr::kable(head(mvData$clusters))
knitr::kable(head(mvData))
obj <- dice(hgsc, nk = 3, reps = 10, algorithms = c("km","hc","diana"),
cons.funs = c("majority"))
knitr::kable(head(obj$clusters))
obj <- dice(dat, nk = 3, reps = 10, algorithms = c("km","hc","diana"),
cons.funs = c("majority"))
knitr::kable(head(obj$clusters))
head(obj$clusters)
obj$clusters
obj
knitr::kable(obj$indices$ii$`3`)
x <- consensus_cluster(dat, nk = 3, reps = 10, algorithms = c("km","hc","diana"),
progress = FALSE)
x
km.3 <- x[, , "KM_Euclidean", "3", drop = FALSE]
cm <- consensus_matrix(km.3)
dim(cm)
hm <- graph_heatmap(km.3)
km.3
head(km.3)
km.3
x
km.3 <- x[, , "HC_Euclidean", "3", drop = FALSE]
cm <- consensus_matrix(km.3)
dim(cm)
hm <- graph_heatmap(km.3)
ccomb_matrix <- consensus_combine(x, element = "matrix")
ccomb_matrix
ccomb_class <- consensus_combine(x, element = "class")
ccomb_class
str(ccomb_matrix, max.level = 2)
str(ccomb_matrix, max.level = 3)
str(ccomb_matrix, max.level = 2)
cm_k3 <- consensus_matrix(ccomb_class$`3`)
cm_k3
ccomp <- consensus_evaluate(hgsc, x, plot = FALSE)
ccomp <- consensus_evaluate(dat, x, plot = FALSE)
ccomp
ccomp <- consensus_evaluate(dat, x, plot = TRUE)
ccomp <- consensus_evaluate(dat, x, plot = TRUE)
cm_k3
table(cm_k3, iris$Species)
mvData
cm_k3$cluster
x
x$cluster
ccomb_class
cm_k3
ccomb_class2 <- consensus_combine(cm_k3, element = "class")
cm_k3
ccomb_class2 <- consensus_combine(ccomb_class$`3`, element = "class")
cm_k3 <- consensus_matrix(ccomb_class$`3`)
cm_k3
ccomb_class
cm_k3 <- consensus_matrix(ccomb_class)
cm_k3
ccomb_class <- consensus_combine(x, element = "class")
ccomb_class
library(diceR)
library(dplyr)
library(ggplot2)
library(diceR)
library(dplyr)
library(ggplot2)
library(pander)
data(hgsc)
hgsc <- hgsc[1:100, 1:50]
#agrupar os hgscdados em 3 ou 4 clusters, usando 80% de reimampling em 5 repeti??es,
#para esses algoritmos de cluster: Hierarchical Clustering, PAM e DIvisive
#ANAlysis Clustering (DIANA).
#A dist?ncia euclidiana ? utilizada para todos os algoritmos.
CC <- consensus_cluster(hgsc, nk = 3:4, p.item = 0.8, reps = 5,
algorithms = c("hc", "pam", "diana"))
co <- capture.output(str(CC))
strwrap(co, width = 80)
CC <- apply(CC, 2:4, impute_knn, data = hgsc, seed = 1)
bigger = options()$max.print  # or add something larger
options("max.print" = bigger) # apparently RStudio sets max.print very low.
CC
# Filtra somente PAM_Euclidean 4
pam.4 <- CC[, , "PAM_Euclidean", "4", drop = FALSE]
cm <- consensus_matrix(pam.4)
dim(cm)
dev.off()
hm <- graph_heatmap(pam.4)
dev.off()
ccomb_matrix <- consensus_combine(CC, element = "matrix")
ccomb_class <- consensus_combine(CC, element = "class")
ccomb_matrix
ccomb_class
str(ccomb_matrix, max.level  = 5)
cm_k3 <- consensus_matrix(ccomb_class$`3`)
cm_k3
cm_k4 <- consensus_matrix(ccomb_class$`4`)
x <- consensus_cluster(dat, nk = 3, reps = 10, algorithms = c("km","hc","diana"),
progress = FALSE)
library(diceR)
library(ggplot2)
dat <- iris[, -5]
head(iris)
head(dat)
x <- consensus_cluster(dat, nk = 3, reps = 10, algorithms = c("km"),
progress = FALSE)
x <- consensus_cluster(dat, nk = 3, reps = 10, algorithms = c("km","hc","diana"),
progress = FALSE)
ccomp <- consensus_evaluate(dat, x, plot = TRUE)
x <- consensus_cluster(dat, nk = 3, reps = 10, algorithms = c("km","hc","diana"),
progress = FALSE, plot = TRUE )
x <- consensus_cluster(dat, nk = 3, reps = 10, algorithms = c("km","hc","diana"),
progress = FALSE, plot(progress=TRUE) )
x <- consensus_cluster(dat, nk = 3, reps = 10, algorithms = c("km","hc","diana"),
progress = FALSE )
km.3 <- CC[, , "KM_Euclidean", "3", drop = FALSE]
km.3 <- x[, , "KM_Euclidean", "3", drop = FALSE]
cm <- consensus_matrix(km.3)
dim(cm)
hm <- graph_heatmap(km.3)
km.3 <- x[, , "HC_Euclidean", "3", drop = FALSE]
cm <- consensus_matrix(km.3)
dim(cm)
cm
hm <- graph_heatmap(km.3)
km.3 <- x[, , "HC_Euclidean", "3", drop = FALSE]
cm <- consensus_matrix(km.3)
dim(cm)
hm <- graph_heatmap(km.3)
ccomp <- consensus_evaluate(dat, x, plot = TRUE)
ccomp <- consensus_evaluate(dat, x, plot = TRUE)
x <- consensus_cluster(dat, nk = 3, reps = 5, algorithms = c("km","hc","diana"),
progress = FALSE )
km.3 <- x[, , "HC_Euclidean", "3", drop = FALSE]
cm <- consensus_matrix(km.3)
dim(cm)
hm <- graph_heatmap(km.3)
ccomp <- consensus_evaluate(dat, x, plot = TRUE)
ccomp <- consensus_evaluate(dat, x, plot = TRUE)
ccomb_matrix <- consensus_combine(CC, element = "matrix")
ccomb_class <- consensus_combine(CC, element = "class")
ccomb_matrix <- consensus_combine(x, element = "matrix")
ccomb_class <- consensus_combine(x, element = "class")
hm <- graph_heatmap(ccomb_matrix)
hm <- graph_heatmap(ccomb_class)
ccomb_class
cm_all <- consensus_matrix(ccomb_class)
cm_all
hm <- graph_heatmap(cm_all)
cm_all
hm <- graph_heatmap(cm_all)
# Plot CDF
p <- graph_cdf(CC1)
data(hgsc)
dat <- hgsc[1:100, 1:50]
x <- consensus_cluster(dat, nk = 4, reps = 4, algorithms = c("hc", "diana"),
progress = FALSE)
CSPA(x, k = 4)
# Consensus clustering for 3 algorithms
library(ggplot2)
set.seed(911)
x <- matrix(rnorm(80), ncol = 10)
CC1 <- consensus_cluster(x, nk = 2:4, reps = 3,
algorithms = c("hc", "pam", "km"), progress = FALSE)
# Plot CDF
p <- graph_cdf(CC1)
# Plot CDF
p <- graph_cdf(CC1)
CC1
# Plot CDF
p <- graph_cdf(CC1)
# Change y label and add colours
p + labs(y = "Probability") + stat_ecdf(aes(colour = k)) +
scale_color_brewer(palette = "Set2")
library(diceR)
library(dplyr)
library(ggplot2)
library(pander)
data(hgsc)
hgsc <- hgsc[1:100, 1:50]
#agrupar os hgscdados em 3 ou 4 clusters, usando 80% de reimampling em 5 repeti??es,
#para esses algoritmos de cluster: Hierarchical Clustering, PAM e DIvisive
#ANAlysis Clustering (DIANA).
#A dist?ncia euclidiana ? utilizada para todos os algoritmos.
CC <- consensus_cluster(hgsc, nk = 3:4, p.item = 0.8, reps = 5,
algorithms = c("hc", "pam", "diana"))
co <- capture.output(str(CC))
strwrap(co, width = 80)
CC <- apply(CC, 2:4, impute_knn, data = hgsc, seed = 1)
bigger = options()$max.print  # or add something larger
options("max.print" = bigger) # apparently RStudio sets max.print very low.
CC
# Filtra somente PAM_Euclidean 4
pam.4 <- CC[, , "PAM_Euclidean", "4", drop = FALSE]
cm <- consensus_matrix(pam.4)
dim(cm)
dev.off()
hm <- graph_heatmap(pam.4)
ccomb_matrix <- consensus_combine(CC, element = "matrix")
ccomb_class <- consensus_combine(CC, element = "class")
str(ccomb_matrix, max.level  = 5)
cm_k3 <- consensus_matrix(ccomb_class$`3`)
cm_k4 <- consensus_matrix(ccomb_class$`4`)
# consensus matrix across subsamples and algorithms and k
cm_all <- consensus_matrix(ccomb_class)
CC2 <- consensus_cluster(hgsc, nk = 3:4, p.item = 0.8, reps = 5,
algorithms = "km")
ccomb_class2 <- consensus_combine(CC, CC2, element = "class")
ccomp <- consensus_evaluate(hgsc, CC, CC2, plot = FALSE)
ctrim <- consensus_evaluate(hgsc, CC, CC2, trim = TRUE, reweigh = FALSE, n = 2)
data(hgsc)
dat <- hgsc[1:100, 1:50]
x <- consensus_cluster(dat, nk = 4, reps = 4, algorithms = c("hc", "diana"),
progress = FALSE)
CSPA(x, k = 4)
# Consensus clustering for 3 algorithms
library(ggplot2)
set.seed(911)
x <- matrix(rnorm(80), ncol = 10)
CC1 <- consensus_cluster(x, nk = 2:4, reps = 3,
algorithms = c("hc", "pam", "km"), progress = FALSE)
# Plot CDF
p <- graph_cdf(CC1)
# Change y label and add colours
p + labs(y = "Probability") + stat_ecdf(aes(colour = k)) +
scale_color_brewer(palette = "Set2")
# Delta Area
p <- graph_delta_area(CC1)
# Heatmaps with column side colours corresponding to clusters
CC2 <- consensus_cluster(x, nk = 3, reps = 3, algorithms = "hc", progress =
FALSE)
graph_heatmap(CC2)
# Track how cluster assignments change between algorithms
p <- graph_tracking(CC1)
data(hgsc)
library(diceR)
library(dplyr)
library(ggplot2)
library(pander)
data(hgsc)
hgsc <- hgsc[1:100, 1:50]
#agrupar os hgscdados em 3 ou 4 clusters, usando 80% de reimampling em 5 repeti??es,
#para esses algoritmos de cluster: Hierarchical Clustering, PAM e DIvisive
#ANAlysis Clustering (DIANA).
#A dist?ncia euclidiana ? utilizada para todos os algoritmos.
CC <- consensus_cluster(hgsc, nk = 3:4, p.item = 0.8, reps = 5,
algorithms = c("hc", "pam", "diana"))
co <- capture.output(str(CC))
strwrap(co, width = 80)
CC <- apply(CC, 2:4, impute_knn, data = hgsc, seed = 1)
bigger = options()$max.print  # or add something larger
options("max.print" = bigger) # apparently RStudio sets max.print very low.
CC
# Filtra somente PAM_Euclidean 4
pam.4 <- CC[, , "PAM_Euclidean", "4", drop = FALSE]
cm <- consensus_matrix(pam.4)
dim(cm)
dev.off()
hm <- graph_heatmap(pam.4)
ccomb_matrix <- consensus_combine(CC, element = "matrix")
ccomb_class <- consensus_combine(CC, element = "class")
str(ccomb_matrix, max.level  = 5)
cm_k3 <- consensus_matrix(ccomb_class$`3`)
cm_k4 <- consensus_matrix(ccomb_class$`4`)
# consensus matrix across subsamples and algorithms and k
cm_all <- consensus_matrix(ccomb_class)
CC2 <- consensus_cluster(hgsc, nk = 3:4, p.item = 0.8, reps = 5,
algorithms = "km")
ccomb_class2 <- consensus_combine(CC, CC2, element = "class")
ccomp <- consensus_evaluate(hgsc, CC, CC2, plot = FALSE)
ctrim <- consensus_evaluate(hgsc, CC, CC2, trim = TRUE, reweigh = FALSE, n = 2)
data(hgsc)
dat <- hgsc[1:100, 1:50]
x <- consensus_cluster(dat, nk = 4, reps = 4, algorithms = c("hc", "diana"),
progress = FALSE)
CSPA(x, k = 4)
# Consensus clustering for 3 algorithms
library(ggplot2)
set.seed(911)
x <- matrix(rnorm(80), ncol = 10)
CC1 <- consensus_cluster(x, nk = 2:4, reps = 3,
algorithms = c("hc", "pam", "km"), progress = FALSE)
# Plot CDF
p <- graph_cdf(CC1)
CC1
# Consensus clustering for 3 algorithms
library(ggplot2)
set.seed(911)
x <- matrix(rnorm(80), ncol = 10)
CC1 <- consensus_cluster(x, nk = 2:4, reps = 3,
algorithms = c("hc", "pam", "km"), progress = FALSE)
# Plot CDF
p <- graph_cdf(CC1)
dev.off()
# Plot CDF
p <- graph_cdf(CC1)
CC1
# Change y label and add colours
p + labs(y = "Probability") + stat_ecdf(aes(colour = k)) +
scale_color_brewer(palette = "Set2")
# Plot CDF
p <- graph_cdf(CC1)
# Change y label and add colours
p + labs(y = "Probability") + stat_ecdf(aes(colour = k)) +
scale_color_brewer(palette = "Set2")
# Delta Area
p <- graph_delta_area(CC1)
dev.off()
# Plot CDF
p <- graph_cdf(CC1)
# Heatmaps with column side colours corresponding to clusters
CC2 <- consensus_cluster(x, nk = 3, reps = 3, algorithms = "hc", progress =
FALSE)
graph_heatmap(CC2)
# Track how cluster assignments change between algorithms
p <- graph_tracking(CC1)
dev.off()
# Track how cluster assignments change between algorithms
p <- graph_tracking(CC1)
table(majority_voting(cc[, , 1, 1, drop = FALSE], is.relabelled = FALSE))
data(hgsc)
dat <- hgsc[1:100, 1:50]
cc <- consensus_cluster(dat, nk = 4, reps = 6, algorithms = "pam", progress =
FALSE)
table(majority_voting(cc[, , 1, 1, drop = FALSE], is.relabelled = FALSE))
x <- consensus_cluster(dat, nk = 3, reps = 5, algorithms = c("km","hc","diana"),
progress = FALSE )
km.3 <- x[, , "HC_Euclidean", "3", drop = FALSE]
cm <- consensus_matrix(km.3)
dim(cm)
library(diceR)
library(ggplot2)
dat <- iris[, -5]
head(iris)
head(dat)
x <- consensus_cluster(dat, nk = 3, reps = 5, algorithms = c("km","hc","diana"),
progress = FALSE )
km.3 <- x[, , "HC_Euclidean", "3", drop = FALSE]
cm <- consensus_matrix(km.3)
dim(cm)
hm <- graph_heatmap(km.3)
ccomp <- consensus_evaluate(dat, x, plot = TRUE)
ccomb_matrix <- consensus_combine(x, element = "matrix")
ccomb_class <- consensus_combine(x, element = "class")
hm <- graph_heatmap(ccomb_class)
x <- consensus_cluster(dat, nk = 3, reps = 5, algorithms = c("km","hc","diana"),
progress = FALSE )
dev.off()
# Plot CDF
p <- graph_cdf(x)
# Change y label and add colours
p + labs(y = "Probability") + stat_ecdf(aes(colour = k)) +
scale_color_brewer(palette = "Set2")
# Delta Area
p <- graph_delta_area(x)
km.3 <- x[, , "HC_Euclidean", "3", drop = FALSE]
cm <- consensus_matrix(km.3)
dim(cm)
hm <- graph_heatmap(km.3)
cm_all
x <- consensus_cluster(dat, nk = 3, reps = 5, algorithms = c("km","hc","diana"),
progress = FALSE )
dev.off()
# Plot CDF
p <- graph_cdf(x)
# Change y label and add colours
p + labs(y = "Probability") + stat_ecdf(aes(colour = k)) +
scale_color_brewer(palette = "Set2")
# Delta Area
p <- graph_delta_area(x)
km.3 <- x[, , "HC_Euclidean", "3", drop = FALSE]
cm <- consensus_matrix(km.3)
dim(cm)
hm <- graph_heatmap(km.3)
